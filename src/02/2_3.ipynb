{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input layerの入力表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 流れ\n",
    "\n",
    "1. パッチへ分割\n",
    "   1. 画像をパッチに分割する\n",
    "   2. flattenして、パッチの縦×横×チャンネルの長さのベクトルに変換する\n",
    "2. パッチ埋め込み\n",
    "   1. 1層の線形層で良いベクトルに変換する\n",
    "   2. 良いベクトル:=損失が少ないベクトル\n",
    "3. クラストークンを定義\n",
    "   1. 画像全体の情報を保持する\n",
    "   2. パッチ埋め込みのベクトルの大きさのベクトル\n",
    "   3. 標準正規分布に従った値を設定\n",
    "   4. クラストークンは学習のパラメータ\n",
    "4. 位置埋め込み\n",
    "   1. パッチ埋め込みだけではパッチの位置情報がないため、位置をクラストークンとパッチ埋め込みベクトルに埋め込む\n",
    "   2. 初期値は標準正規分布に従う乱数を設定\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数式表現\n",
    "\n",
    "### 1. 入力画像からflattenまで\n",
    "\n",
    "> $H$: 画像の高さ（pixel）\n",
    ">\n",
    "> $W$: 画像の幅（pixel）\n",
    ">\n",
    "> $C$: 色情報（RGB）\n",
    ">\n",
    "> $N_p$: パッチの個数 \n",
    ">\n",
    "> $P$: パッチの縦横のサイズ（pixel）\n",
    "\n",
    "\n",
    "とすると入力画像を\n",
    "\n",
    "> $\\boldsymbol{x} \\in \\mathbb{R}^{H \\times W \\times C}$\n",
    "\n",
    "\n",
    "flattenした入力画像を\n",
    "\n",
    "> $\\boldsymbol{x_p} \\in \\mathbb{R}^{N_p\\times(P^2 \\cdot C)}$\n",
    "\n",
    "と表せる。\n",
    "\n",
    "flattenした各バッチのベクトルは\n",
    "> $x_p^1, x_p^2, \\dots, x_p^{N_p}$\n",
    "\n",
    "と表す。\n",
    "\n",
    "\n",
    "### 2. パッチ埋め込み\n",
    "\n",
    "> $D$ :「$\\boldsymbol{x_p}$ を埋め込んだベクトルの次元」\n",
    "\n",
    "とすると、$\\boldsymbol{x_p}$を埋め込む線形層の重みは\n",
    "\n",
    "> $E \\in \\mathbb{R}^{(P^2 \\cdot C) \\times D} $ \n",
    "\n",
    "と表せ、各バッチに$E$を適用すると\n",
    "\n",
    "> $x_{p}^i E \\in \\mathbb{R}^D \\quad (i = 1, 2, \\dots, N_p)$ \n",
    "\n",
    "であり、\n",
    "\n",
    "> $x_p E = [ x_p^1 E; x_p^2 E; \\dots, x_p^{N_p} E ] \\in \\mathbb{R}^{N_p \\times D}$\n",
    "\n",
    "である。\n",
    "\n",
    "\n",
    "### 3. クラストークン\n",
    "\n",
    "\n",
    "$x_p E$ にクラストークンを付加する。$x_{class} \\in \\mathbb{R}^D$ を$x_p E$に追加して、\n",
    "\n",
    "> $x_{p+t} E = [ x_{class}; x_p^1E;  x_p^2E; \\dots, x_p^{N_p}E] \\in \\mathbb{R}^{(N_p + 1) \\times D}$\n",
    "\n",
    "である。\n",
    "\n",
    "### 4. 位置埋め込み\n",
    "\n",
    "> トークン数:「クラストークン+パッチ数」\n",
    "\n",
    "とすると、トークン数は$N_p + 1$である。\n",
    "\n",
    "位置埋め込みはD次元ベクトルがトークン数、すなわち\n",
    "\n",
    "> $E_{pos} = [E x_{pos}^{class}; E x_{pos}^1; E x_{pos}^2; \\dots; E x_{pos}^{N_p}] \\in \\mathbb{R}^{(N_p + 1) \\times D}$\n",
    "\n",
    "であるから、位置埋め込みしたEncoderへの入力$z_0$は\n",
    "\n",
    "> $z_0 = x_{p+t}E + E_{pos} \\in \\mathbb{R}^{(N_p + 1) \\times D} $\n",
    "\n",
    "である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Layerの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (17) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 105\u001b[0m\n\u001b[1;32m    103\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(batch_size, channel, height, weight)\n\u001b[1;32m    104\u001b[0m input_layer \u001b[39m=\u001b[39m VitInputLayer(num_patch_row\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 105\u001b[0m z_0 \u001b[39m=\u001b[39m input_layer(x)\n\u001b[1;32m    107\u001b[0m \u001b[39mprint\u001b[39m(z_0\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.asdf/installs/python/miniforge3-4.10.3-10/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [9], line 94\u001b[0m, in \u001b[0;36mVitInputLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m z_0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[1;32m     89\u001b[0m     [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_token\u001b[39m.\u001b[39mrepeat(repeats\u001b[39m=\u001b[39m(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)), z_0],\n\u001b[1;32m     90\u001b[0m     dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     91\u001b[0m )\n\u001b[1;32m     93\u001b[0m \u001b[39m# 位置埋め込み\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m z_0 \u001b[39m=\u001b[39m z_0 \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_emb\n\u001b[1;32m     96\u001b[0m \u001b[39mreturn\u001b[39;00m z_0\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (17) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# emb: 埋め込み。embedded\n",
    "\n",
    "class VitInputLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        emb_dim: int = 384,\n",
    "        num_patch_row: int = 2,\n",
    "        image_size: int = 32\n",
    "    ):\n",
    "        \"\"\"        \n",
    "        Args:\n",
    "            in_channels (int, optional): 入力画像のチャンネル数. Defaults to 3.\n",
    "            emb_dim (int, optional): 埋め込み後のベクトルの長さ. Defaults to 384.\n",
    "            num_patch_row (int, optional): 高さ方向のパッチ数. Defaults to 2.\n",
    "            image_size (int, optional): 入力画像の1辺の大きさ. Defaults to 32.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_patch_row = num_patch_row\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # パッチ数\n",
    "        self.num_patch = self.num_patch_row ** 2\n",
    "\n",
    "        # パッチの大きさ\n",
    "        self.patch_size = int(self.image_size // self.num_patch_row)\n",
    "\n",
    "        # 乳画像のパッチへの分割 & パッチ埋め込みを一気に行う層\n",
    "        self.patch_emb_layer = nn.Conv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.emb_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size\n",
    "        )\n",
    "\n",
    "        # クラストークン\n",
    "        self.cls_token = nn.parameter.Parameter(\n",
    "                torch.randn(1,1, emb_dim)\n",
    "            \n",
    "        )\n",
    "\n",
    "        # 位置埋め込み\n",
    "        # トークン数(パッチ数+クラストークン数(1))\n",
    "        num_token = self.patch_size + 1\n",
    "        self.pos_emb = nn.parameter.Parameter(\n",
    "            torch.randn(1, self.num_patch+1, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"前処理\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): （B, C, H, W）\n",
    "                B: バッチサイズ\n",
    "                C: チャンネル数\n",
    "                H: 高さ\n",
    "                W: 幅\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: ViTへの入力。(B, N, D)\n",
    "                B: バッチサイズ\n",
    "                N: トークン数\n",
    "                D: 埋め込みベクトルの長さ\n",
    "        \"\"\"\n",
    "\n",
    "        # パッチの埋め込み\n",
    "\n",
    "        ## P: パッチの1辺のサイズ\n",
    "        ## flattenはパッチ埋め込みの後\n",
    "\n",
    "        ## パッチ埋め込み (B, C, H, W) -> (B, D, H/P, W/P)\n",
    "        z_0 = self.patch_emb_layer(x)\n",
    "\n",
    "        ## flatten (B, D, H/P, W/P) -> (B, D, Np)\n",
    "        ## Np はパッチ数 (= H*W / P**2)\n",
    "        z_0 = z_0.flatten(2)\n",
    "\n",
    "        ## 軸の順番を変更 (B, D, Np) -> (B, Np, D)\n",
    "        z_0 = z_0.transpose(1, 2)\n",
    "\n",
    "        # クラストークンを結合 (B, Np, D) -> (B, N, D)\n",
    "        # cls_token: (1, 1, D) から (B, 1, D)に変換して結合\n",
    "        z_0 = torch.cat(\n",
    "            [self.cls_token.repeat(repeats=(x.size(0), 1, 1)), z_0],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        # 位置埋め込み\n",
    "        z_0 = z_0 + self.pos_emb\n",
    "\n",
    "        return z_0\n",
    "\n",
    "batch_size = 2\n",
    "channel = 3\n",
    "height = 32\n",
    "weight = 32\n",
    "\n",
    "x = torch.randn(batch_size, channel, height, weight)\n",
    "input_layer = VitInputLayer(num_patch_row=2)\n",
    "z_0 = input_layer(x)\n",
    "\n",
    "print(z_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9585, -1.3804]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21a447d43d212849e540c6c38cb6eb2460dc2380e24f4b0de591296981a71bc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
