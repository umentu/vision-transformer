{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Layerでつくったパッチをよりよいパッチにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'height': 181.54999999999998, 'weight': 73.94999999999999, 'age': 34.85}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self-attention の説明を読んで、雰囲気の例。Self-Attention自体を表すものではない。\n",
    "from random import seed, randint\n",
    "\n",
    "\n",
    "class Human:\n",
    "    def __init__(self, height, weight, age):\n",
    "        self.height = height\n",
    "        self.weight = weight\n",
    "        self.age = age\n",
    "\n",
    "seed(0)\n",
    "a = Human(randint(160, 200), randint(60, 80), randint(20, 80))\n",
    "b = Human(randint(160, 200), randint(60, 80), randint(20, 80))\n",
    "c = Human(randint(160, 200), randint(60, 80), randint(20, 80))\n",
    "d = Human(randint(160, 200), randint(60, 80), randint(20, 80))\n",
    "\n",
    "# a と a, b, c, dがどれくらい似ているかを数値化する\n",
    "# 特徴量 = 各Humanごとの慎重・体重・年齢を足した値\n",
    "# 類似度 = そのHumanの特徴量 / aの特徴量\n",
    "\n",
    "features = [ sum([h.__dict__[k] for k in h.__dict__]) for h in (a, b, c, d) ]\n",
    "\n",
    "# 乱数を固定しているので、feature = [279, 303, 304, 337]\n",
    "# aに一番近いのは a でその次に　aに近いのは b。\n",
    "\n",
    "# 一番近いaを60%、二番目近いbを30%、あとの二つを5%の重みづけでaを更新する\n",
    "\n",
    "new_a = Human(\n",
    "    weight=0.6*a.weight+0.3*b.weight+0.05*(c.weight + d.weight),\n",
    "    height=0.6*a.height+0.3*b.height+0.05*(c.height + d.height),\n",
    "    age=0.6*a.age+0.3*b.age+0.05*(c.age + d.age),\n",
    ")\n",
    "\n",
    "new_a.__dict__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 流れ\n",
    "\n",
    "1. 埋め込み\n",
    "   1. Input Layerのベクトルを別々の線形層でqkv(Query, Key, Value)を作成\n",
    "2. 内積\n",
    "   1. q, kの行列積$qk^T$を計算\n",
    "      1. $q^Tk$ではなく$qt^K$なのは、行方向にデータを保持しているから\n",
    "   2. $qk^T$にSoftMax関数を適用して正規化する\n",
    "   3. $A = softmax(\\frac{qk^T}{\\sqrt{D_h}})$ をAttention Weightという\n",
    "3. の加重和\n",
    "   1. $A$にvを加重和\n",
    "   2. $Av$をScaled Dot-Product Attention という\n",
    "4. MHSA(Multi-Head Self-Attention)\n",
    "   1. 「1.埋め込み」で作成したq,k,vをヘッド数?の数に分割する\n",
    "   2. 分割したq,k,vに内積して、複数のAttention Weightを作成する\n",
    "   3. 各Attention Weightと分割したと各vを加重和して、複数のScaled Dot-Prodct Attentionを作成する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数式表現\n",
    "\n",
    "### 1. 埋め込み\n",
    "> $z \\in \\R^{N \\times D}$\n",
    "\n",
    "をSelf-Attentionの入力とする（$z_0$と同じ大きさ）。\n",
    "\n",
    "$z$をq,k,vに埋め込むときのベクトルの長さを$D_h$とするととしてq,k,vの線形層はそれぞれ$W^q, W^k, W^v \\in \\mathbb{R}^{D \\times D_h}$とすれば、\n",
    "\n",
    "> $q = z W^q \\in \\mathbb{R}^{N \\times D_h}$\n",
    "> \n",
    "> $k = z W^q \\in \\mathbb{R}^{N \\times D_h}$\n",
    "> \n",
    "> $v = z W^q \\in \\mathbb{R}^{N \\times D_h}$\n",
    "\n",
    "と表せる。\n",
    "\n",
    "### 2. 内積\n",
    "\n",
    "$q$と$k$の行列積 $ qk^T \\in \\mathbb{R}^{N \\times N}$ に SoftMax関数を適用する。\n",
    "\n",
    "$$A = softmax(\\frac{qk^T}{\\sqrt{D_h}}) \\in \\mathbb{R}^{N \\times N}$$\n",
    "\n",
    "\n",
    "## 3. 加重和\n",
    "\n",
    "$A$に$v$を加重和して、Scaled Dot-Product Attentionを作成する。\n",
    "\n",
    "$$SA = Av = softmax(\\frac{qk^T}{\\sqrt{D_h}}) v \\in \\mathbb{R}^{N \\times D_h}$$\n",
    "\n",
    "\n",
    "## MHSA\n",
    "\n",
    "1-3 と同様の操作をして、Scaled Dot-Product Attention を作成する。\n",
    "\n",
    "$q = W^q z,k = W^k z,v = W^v z$ を作成するところは同じ。\n",
    "\n",
    "少しだけ復習すると、$z$の形は次のような行列である。\n",
    "\n",
    "| | $vec1$ |  $vec2$ | $\\dots$ | $vecD$ |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| CLS | $cls1$ | $cls2$  | $\\dots$ | $clsD$ |\n",
    "| $patch_1$ | $pat_{1}1$ | $pat_{1}2$ | $\\dots$ | $pat_{1}D$ |\n",
    "| $patch_2$ | $pat_{2}1$ | $pat_{2}2$ | $\\dots$ | $pat_{2}D$ |\n",
    "|  $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\vdots$ \n",
    "| $patch_{N}$ | $pat_{N}1$ | $pat_{N}2$ | $\\dots$ | $pat_{N}D$\n",
    "\n",
    "CLSと$N$個のパッチが行ベクトルで並んでいる。\n",
    "これらを線形層$W^q, W^k, W^v$で$q,k,v$で埋め込み、$D_h$次にする。\n",
    "\n",
    "$q,k,v$はすべて同じ形をしているため、$q$の場合だけを明記する。\n",
    "\n",
    "| | $q1$ |  $q2$ | $\\dots$ | $q(D_h)$ |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| CLS | $cls1$ | $cls2$  | $\\dots$ | $clsD_h$ |\n",
    "| $q_1$ | $q_{1}1$ | $q_{1}2$ | $\\dots$ | $q_{1}D_h$ |\n",
    "| $q_2$ | $q_{2}1$ | $q_{2}2$ | $\\dots$ | $q_{2}D_h$ |\n",
    "|  $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\vdots$ \n",
    "| $q_{N}$ | $q_{N}1$ | $q_{N}2$ | $\\dots$ | $q_{N}D_h$\n",
    "\n",
    "これを **列** で $K$個に分割する。すなわち、その$i$番目の分割は次のようになる。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$q, k, v$を$K$個に分割する。\n",
    "\n",
    "$vec(i1)$ | $vec(i2)$  | $\\dots$ | $vec(i(D_h / K))$  \n",
    "  ---     |   ---      |  ---  |  ---  \n",
    "$cls(i1)$ | $cls(i2)$  | $\\dots$ | $cls(i(D_h / K))$  \n",
    "$pat_1(i1)$ | $pat_1(i2)$  | $\\dots$ | $pat_1(i(D_h / K))$  \n",
    "$pat_2(i1)$ | $pat_2(i2)$  | $\\dots$ | $pat_2(i(D_h / K))$  \n",
    " $\\vdots$ |  $\\vdots$ |  $\\vdots$ |  $\\vdots$ \n",
    "$pat_{N}(i1)$ | $pat_{N}(i2)$  | $\\dots$ | $pat_{N}(i(D_h / K))$  \n",
    "\n",
    "元のパッチ埋め込みの次数が$D_h$でそれを$K$個に分割しているため、分割した一つは$D_h / K$個になる。\n",
    "\n",
    "$q, k, v$をそれぞれ$K$個に分割すると\n",
    "\n",
    "> $[q_1, q_2, \\dots, q_K], \\quad q_i \\in  \\mathbb{R}^{N \\times \\frac{D_h}{K}} $\n",
    "> \n",
    "> $[k_1, k_2, \\dots, k_K], \\quad k_i \\in  \\mathbb{R}^{N \\times \\frac{D_h}{K}} $\n",
    "> \n",
    "> $[q_1, q_2, \\dots, q_K], \\quad q_i \\in  \\mathbb{R}^{N \\times \\frac{D_h}{K}} $\n",
    "\n",
    "$i$ 番目の$q_i$と$k_i$の内積をとると、$q_i, k_i \\in \\mathbb{R}^{N \\times \\frac{D}{K}}$ であるから、\n",
    "\n",
    "$$q_i k_i^T \\in \\mathbb{R}^{N \\times N}$$\n",
    "\n",
    "と、**通常のSelf-Attentionと同じ大きさである**。これにsoftmax関数を適用して正規化する。\n",
    "\n",
    "$$A_i = softmax(\\frac{q_i k_i^T}{\\sqrt{D_h} }), \\quad i = 1, 2, \\dots, K$$\n",
    "\n",
    "これらを加重和して、Scaled Dot-Product Attention を作成する。\n",
    "\n",
    "$$A_i v_i \\in \\mathbb{R}^{N \\times D_h}, \\quad i = 1, 2, \\dots, K $$\n",
    "\n",
    "これらをまとめて処理する関数を$SA_i(z)$ とすると、\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "SA_i(z): \\mathbb{R}^{N \\times D} & \\stackrel{f}{\\longrightarrow} & \\mathbb{R}^{N \\times N} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "であり、\n",
    "\n",
    "$$\n",
    "SA_i(z) = sofmax(\\frac{q_i k_i^T}{ \\sqrt{D_h} } ) v_i\n",
    "$$\n",
    "\n",
    "である。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21a447d43d212849e540c6c38cb6eb2460dc2380e24f4b0de591296981a71bc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
