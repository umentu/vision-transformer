{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViTのまとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ViTの全容は次の通り。MHSAなどは雑なまとめ。\n",
    "\n",
    "![](./images/ViT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （ディープラーニングを実装する上での）テンソル（行列）の基礎\n",
    "\n",
    "**テンソルや行列についてご承知の方は飛ばしていただいて構わない。**\n",
    "\n",
    "最初に、テンソルについて紹介する。テンソルとはいわゆる行列を拡張した概念である。\n",
    "行列とは、縦横に並んだ数の集まりである。横の並びを**行**といい、縦の並びを**列**と言う。\n",
    "\n",
    "$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3\\\\\n",
    "4 & 5 & 6\\\\\n",
    "7 & 8 & 9\n",
    "\\end{pmatrix}\n",
    "\\quad\n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "4\\\\\n",
    "7\n",
    "\\end{pmatrix}\n",
    "\\quad\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3\\\\\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "特に2つめや3つめのように、列か行が1つのときは **ベクトル**という。\n",
    "2つめのように列が1つのベクトルを**列ベクトル**、3つめのように行が1つのベクトルを**行ベクトル**という。\n",
    "\n",
    "行列は$A$や$W$のような大文字が、ベクトルは$a$や$x$のような小文字で表されることが多い。\n",
    "\n",
    "\n",
    "少し見方を変えてみる。さきほどの説明では数が並んだものを行列、行数か列数が1のときをベクトルと呼んだ。\n",
    "これを逆に、数が1行もしくは1列のものをベクトルと呼び、それらを縦もしくは横に並んだものを行列と考えることにする。\n",
    "今度は縦横に並んだ同じ大きさの行列を **重ねてみる**。ちょうど、線を並べたら平面、面を並べたら立体ができるようなイメージである。\n",
    "このように行列を重ねてできたものが **テンソル** である。\n",
    "\n",
    "正確にはこの段階のテンソルを **3階のテンソル**と呼ぶ。この3階のテンソルをさらに重ねたら5階のテンソル、更に……と続く。\n",
    "ちなみに1階のテンソルはベクトル、2階のテンソルは行列のことを指す。\n",
    "\n",
    "プログラミングで考えると、配列構造の多重化で表現できる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = [1, 2, 3]\n",
    "\n",
    "matrix = [\n",
    "    [1, 2, 3], \n",
    "    [4, 5, 6], \n",
    "    [7, 8, 9]\n",
    "]\n",
    "\n",
    "tensor = [\n",
    "    [\n",
    "        [1, 2, 3],\n",
    "        [4, 5, 6],\n",
    "        [7, 8, 9]\n",
    "    ],\n",
    "    [\n",
    "        [10, 11, 12],\n",
    "        [13, 14, 15],\n",
    "        [16, 17, 18]\n",
    "    ],\n",
    "    [\n",
    "        [19, 20, 21],\n",
    "        [22, 23, 24],\n",
    "        [25, 26, 27],\n",
    "\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次元が$D$のベクトル全体の集合を$\\mathbb{R}^D$と表す。$\\mathbb{R}$とは数学で「実数全体の集合」を表している。\n",
    "これがD個並んでいるベクトル、という意味である。\n",
    "\n",
    "行列は$\\mathbb{R}^{N \\times D}$のように表す。これは行数が$N$で列数が$D$の行列全体の集合である。\n",
    "$A \\in \\mathbb{R}^{N \\times D}$と表記された場合は、$A$がN行D列の行列であることを表す。\n",
    "$N$か$D$が1のときはベクトルを表す。ベクトルが行列の特別な場合、というのが見て取れる。\n",
    "\n",
    "テンソルは$\\mathbb{R}^{C \\times H \\times W}$のように表す。H行W列の行列がC個重なっているようなイメージである。\n",
    "\n",
    "$C$が1のときはH行W列の行列全体を表す。行列がテンソルの特別な場合であることが見て取れる。\n",
    "\n",
    "$C$が1で、更に$H$か$W$のいずれかが1のときは、$H$次元もしくは$W$次元ベクトル全体の集合を表す。ベクトルもテンソルの特別な場合である。\n",
    "\n",
    "\n",
    "ViTに限らずディープラーニングでは、頻繁にあるテンソル$X$（行列・ベクトル）からそれとは別の形のテンソル（行列・ベクトル）$Y$に変換することがある。\n",
    "これは、**$X$にとあるテンソル$W$をかけている処理に過ぎない**。\n",
    "\n",
    "行列のかけ算は厳密に説明するとややこしいため、あくまでも「形」だけでみる。\n",
    "$X \\in \\mathbb{R}^{2 \\times 3}$という行列があったときに、これに$W \\in \\mathbb{R}^{3 \\times 5}$という行列をかけることを考える。\n",
    "行列のかけ算は少し特殊で、$X \\times W$ を考えたときに、左側の$X$の列数と$W$の行数が一致しているときにのみ計算できて、行数が$X$の行数で列数が$W$の列数の行列ができる。今回の場合は、$X$ が$ \\mathbb{R}^{2 \\times 3}$ で $W$ が $\\mathbb{R}^{3 \\times 5}$のため、$X$の列数と$W$の行数は5で一致している。そのため、$X \\times W$は計算できてでき上がる行列は$ \\mathbb{R}^{2 \\times 5}$の行列となる。$W \\times X$が計算できないことは、行数と列数をみれば確認できる。\n",
    "\n",
    "ベクトルに行列をかければ、別の次元のベクトルに変換もできる。\n",
    "\n",
    "上で見た通り、ベクトルも行列もテンソルの特別な場合であった。つまり「とあるテンソルを別の「形」のテンソルに変換するときには、なんらかのテンソルをかける」という処理をしているにすぎない。かけるための「なんらかのテンソル」は具体的にはディープラーニングの手法によって定まるものであったり、もしくは既知のものであればPyTorchなどのライブラリですでに用意されている。そのため、開発者側ではあくまでも「形」がどのように変化するかさえ抑えておけば良い。（もちろん、新たなディープラーニングの手法を開発する場合はその限りではない）\n",
    "\n",
    "まとめると、\n",
    "\n",
    "- データはベクトルや行列、テンソルで表現される。ベクトルも行列も、テンソルの特別な場合である\n",
    "- データに**計算できる形の**テンソルをかけることで、目的に近づけていく\n",
    "- ディープラーニングを学ぶ上で、テンソルの形を理解することのみが重要\n",
    "\n",
    "\n",
    "である。\n",
    "\n",
    "ViTの解説中には「線形層」という言葉が出てくるが、これはおおよそ「テンソルをかけている」と解釈すればよい。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解説"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数式表現をここでざっとまとめる。使う記号を先にまとめる。\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "H &: 画像の高さ\\\\\n",
    "W &: 画像の幅\\\\\n",
    "C &: 画像のチャンネル（RGB）\\\\\n",
    "P &: パッチ1つの縦横のサイズ\\\\\n",
    "N_p &: パッチの分割数。(= \\frac{H}{P} * \\frac{W}{P} = \\frac{HW}{P^2})\\\\\n",
    "D &: 各パッチを埋め込み後のベクトルの次元\\\\\n",
    "N &: トークン数。パッチの分割数 + クラストークンの数(1) = N_p + 1\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "### Input Layer\n",
    "\n",
    "![](images/summary/inputlayer.png)\n",
    "\n",
    "$B_n$枚の画像の中から1枚の画像を例に紹介する。\n",
    "\n",
    "画像のサイズを$H \\times W$でチャンネル数（色）を$C=3$とする。\n",
    "このとき、$\\mathbb{R}^{C \\times H \\times W}$と表せる。\n",
    "\n",
    "$\\mathbb{R}^{H \\times W}$は行数が$H$で列数が$W$の行列である。\n",
    "$\\mathbb{R}^{C \\times H \\times W}$ は$H \\times W$の行列が$C$個重なっているというイメージ(いくつかの行列が重なっているものをテンソルという)。\n",
    "今の場合は$C$はRGBの3チャンネルなので、RGBそれぞれの行列がある。\n",
    "Rの行列は行数が$H$で列数が$W$であり、それぞれの要素に0 - 255 の値が入っている。GBも同様。\n",
    "\n",
    "これを **パッチ** に分割する。分割する大きさは個数は場合によって異なる。\n",
    "縦横$P$pixelのパッチに分割すると、縦の分割は$\\frac{H}{P}$個、横の分割は$\\frac{W}{P}$個になる。\n",
    "つまり、パッチの個数は$\\frac{H}{P} \\times \\frac{W}{P} = \\frac{HW}{P^2}$となる。この$\\frac{HW}{P^2}$を$N_p$としておく。\n",
    "\n",
    "分割した各パッチは$\\mathbb{R}^{C \\times \\frac{H}{P} \\times \\frac{W}{P} }$というテンソルの形になる。これも元の画像の時と同じで、$\\frac{H}{P} \\times \\frac{W}{P}$の大きさの行列が$C=3$つ重なっている状態である。個の行列がパッチの個数分、つまり$N_p$個ある。テンソルで表現すれば$\\mathbb{R}^{N_p \\times C \\times \\frac{H}{P} \\times \\frac{W}{P} }$である。\n",
    "\n",
    "これらを **Flatten** でベクトルに変換する。$\\mathbb{R}^{C \\times \\frac{H}{P} \\times \\frac{W}{P} }$ をただの1列の列ベクトルにすると$\\mathbb{R}^{\\frac{CHW}{P^2} }$\n",
    "になる。テンソルの成分をただ一列にしたものと思えば良い。この列ベクトルがパッチの個数分、すなわち$N_p$ある。テンソルで表現すれば$\\mathbb{R}^{N_p \\times C \\times \\frac{CHW}{P^2} }$である。\n",
    "\n",
    "これらのパッチに **パッチ埋め込み**を適用する。埋め込みとは学習に使いやすいベクトルの形に変換することである。今回は返還後のベクトルの次元を$D$とすると、$\\mathbb{R}^{\\frac{CHW}{P^2} }$ から $\\mathbb{R}^{D}$ に埋め込む。この段階でもパッチの個数分、すなわち$N_p$個ある。テンソルで表現すれば$\\mathbb{R}^{N_p \\times D }$である。\n",
    "\n",
    "次にViTにおいて最終的なアウトプットとなる情報である **クラストークン**を今まで計算したテンソルに追加する。クラストークンの大きさはパッチ埋め込みしたときのベクトルの次元と同じ$\\mathbb{R}^D$である。実装的にはパッチ埋め込みした_**テンソルの頭に**_ 追加する。するとパッチの個数にクラストークンを追加した$N_p + 1$個のベクトルができたことになる。$N = N_p + 1$をトークン数と呼ぶ。テンソルの形は$\\mathbb{R}^{N \\times D}$である。\n",
    "\n",
    "Input Layerの最後の工程として **位置埋め込み**をする。位置埋め込みはパッチの位置を保持する埋め込みである。というのも、今までの工程で画像をパッチに分割していろいろやったことで数値で表現しているが、それぞれの数値がどこの画像の位置を表しているかの情報を保持していない。それらを保持するためのテンソルを用意して次の工程に教える役目を持つ。\n",
    "\n",
    "Input Layerはいままでに構成したテンソルと同じ大きさのテンソルを追加する。すなわち、$\\mathbb{R}^{N \\times D}$のテンソル同士を足し合わせるだけなので、大きさは変わらない。\n",
    "\n",
    "### Encoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoderの全容は次の通り。\n",
    "\n",
    "![](images/summary/encoder.png)\n",
    "\n",
    "処理はEncoderBlockごとに分かれる。EncoderBlockの個数はハイパーパラメータである。\n",
    "\n",
    "各**EncoderBlock**で行われる処理は同じで、次で解説する。\n",
    "\n",
    "\n",
    "### EncodeBlock\n",
    "\n",
    "EncoderBlock の全容は次の通り。\n",
    "\n",
    "![](images/summary/encoderblock.png)\n",
    "\n",
    "各項目の詳細な計算方法はそれぞれに紹介するので、ここではおおまかな役割を紹介する。\n",
    "\n",
    "**Layer Normalization** は各データを**正規化**するための方法。正規化とは、データの性質を損なうことなく扱いやすい形にデータを変換することである。「1、100、1000」というデータがあった場合に、単純に大きさだけを考えたい場合は億をとって「1,100,1000」としても順序は変わらない。といった具合に、データを計算しやすい形に整形すること。\n",
    "\n",
    "正規化の方法によってはすべてのデータ（今回の場合はバッチ全体）で計算する必要があるものがあるが、Layer NormalizationはInput Layerで埋め込んだ各ベクトルごとに正規化をするため、それ以外のデータを計算する必要がないのが利点。入力は$\\mathbb{R}^{N \\times D}$で出力も$\\mathbb{R}^{N \\times D}$であるため、変わらない。\n",
    "\n",
    "\n",
    "**MHSA(Multi-Head Self Attention)**は**Self Attention**を複数行う処理。Self Attentionとは、Input Layerで生成した各埋め込みベクトルを**良いデータ**にすることである。良いデータとは「特徴がはっきりしている」といった意味合いである。Self Attentionの特徴として、良いデータを生成するときに **他のデータだけでなく対象のデータ自身も比較対象に加えて** データを生成する。A,B,Cを比較するときに、Aを基準としたらたいていの場合は比較対象としてB,Cのみを比較するが、この比較にA自身も比較の対象に加えるのである。当然Aが一番近いことになるが、次に近いのがCだったときにはAの特徴を一番反映した上でCの特徴を加え、Bは少しだけ反映して新しいAをつくる。これがSelf Attentionで、さらにデータを分割してそれぞれのデータ単位でSelf Attentionをし、データを再度つなぎ合わせるのがMHSAである。\n",
    "\n",
    "MLP(Multi-Layer Perceptron)は、日本語では多層パーセプトロンでいわゆるディープラーニングで使われている多層のニューラルネットワークである。\n",
    "\n",
    "具体的に見ていく。\n",
    "\n",
    "\n",
    "#### Layer Normalization\n",
    "\n",
    "レイヤーの正規化。データ自身の平均と標準偏差を使う。\n",
    "\n",
    "Batch Normalizationという正規化手法もあるが、正規化するときに使う平均と標準偏差がバッチ全体の物を使うため、データごとにトークン数が異なる場合は使えない。\n",
    "Layer Normalizationは各データごとの平均と標準偏差を使うため、そういった不都合な点がない。\n",
    "\n",
    "$a  =  \\begin{pmatrix} a_1\\\\ a_2\\\\ \\vdots\\\\ a_K  \\end{pmatrix}  \\in \\mathbb{R}^K$ に対して（算術）平均$\\mu$と$\\sigma$は\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\mu &= \\frac{1}{K} \\sum_{i=1}^K a_i\\\\\n",
    "\\sigma &= \\sqrt{\\frac{1}{K} \\sum_{i=1}^K (a_i - \\mu)^2}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "である。\n",
    "\n",
    "また、$\\beta_i, \\gamma_i \\in \\mathbb{R}$ を$a$の$i$番目の要素 $a_i$ に対する学習パラメータとすると、 Layer Normalization $LN(a)_i$ は\n",
    "\n",
    "$\n",
    "\\displaystyle LN(a)_i = \\gamma_i \\frac{a_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta_i \\quad (i = 1, 2, \\dots, K)\n",
    "$\n",
    "\n",
    "である。ただし $\\epsilon$は分母が0となることを防ぐための無限小の数値とする。\n",
    "\n",
    "\n",
    "具体的に見てみる。数式が見づらい場合はPythonコードを以下に用意しているので、そちらをみてほしい。\n",
    "\n",
    "画像データをInput Layerに通して生成されたテンソルを\n",
    "$\n",
    "x = \n",
    "\\begin{pmatrix}\n",
    "[x_{1,1} & x_{1,2} & \\dots & x_{1,384}]\\\\\n",
    "[x_{2,1} & x_{2,2} & \\dots & x_{2,384}]\\\\\n",
    "[x_{3,1} & x_{3,2} & \\dots & x_{3,384}]\\\\\n",
    "[x_{4,1} & x_{4,2} & \\dots & x_{4,384}]\\\\\n",
    "[x_{5,1} & x_{5,2} & \\dots & x_{5,384}]\\\\\n",
    "\\end{pmatrix}\n",
    " \\in R^{N \\times D}\n",
    " $\n",
    " ($5$はトークン数（パッチ数+クラストークン）、$384$は各パッチごとにベクトルの長さ)とする。$x$の1列目$p_1=[x_{1,1} \\quad x_{1,2} \\quad \\dots \\quad x_{1,384}]$に\n",
    " Layer Normalizationを適用することを考える。\n",
    "\n",
    "より具体的に見るために $p_1 = [1, 2, 1, 2, 1, 2, \\dots, 1, 2]$だったとすると、平均と標準偏差は\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu &= \\frac{1}{D} (1 + 2 + 1 + 2 + 1+ 2+ \\dots, 1 + 2)\\\\\n",
    "    &= 1.5\\\\\n",
    "\\sigma &= \\sqrt{\\frac{1}{D} ((1 - 3/2)^2 + (2 - 3/2)^2 + (1 - 3/2)^2 + (2 - 3/2)^2 + (1 - 3/2)^2 + (2 - 3/2)^2  + \\dots + (1 - 3/2)^2 + (2 - 3/2)^2 ) }\\\\\n",
    "       &\\fallingdotseq 0.25\n",
    "\\sigma^2 &= 0.0625\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "であるから、\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&[LN(p_1)_1, LN(p_1)_2, \\dots LN(p_1)_D] \\\\\n",
    "&= \n",
    "[\n",
    "\\gamma_1 \\frac{1 - \\frac{3}{2}}{\\sqrt{(0.0625 + \\epsilon)}} + \\beta_1, \n",
    "\\gamma_2 \\frac{2 - \\frac{3}{2}}{\\sqrt{(0.0625 + \\epsilon)}} + \\beta_2, \n",
    "\\dots, \n",
    "\\gamma_{383} \\frac{1 - \\frac{3}{2}}{\\sqrt{(0.0625 + \\epsilon)}} + \\beta_{383},\n",
    "\\gamma_{384} \\frac{2 - \\frac{3}{2}}{\\sqrt{(0.0625 + \\epsilon)}} + \\beta_{384}\n",
    "]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.84450288 -0.55668243  1.84450288 -0.55668243  1.84450288 -0.55668243\n",
      "  1.84450288 -0.55668243  1.84450288 -0.55668243  1.84450288 -0.55668243\n",
      "  1.84450288 -0.55668243  1.84450288 -0.55668243  1.84450288 -0.55668243\n",
      "  1.84450288 -0.55668243  1.84450288 -0.55668243  1.84450288 -0.55668243\n",
      "  1.84450288 -0.55668243  1.84450288 -0.55668243  1.84450288 -0.55668243\n",
      "  1.84450288 -0.55668243  1.84450288 -0.55668243  1.84450288 -0.55668243\n",
      "  1.84450288 -0.55668243  1.84450288 -0.55668243  1.84450288 -0.55668243\n",
      "  1.84450288 -0.55668243  1.84450288 -0.55668243  1.84450288 -0.55668243\n",
      "  1.84450288 -0.55668243  1.84450288 -0.55668243  1.84450288 -0.55668243\n",
      "  1.84450288 -0.55668243  1.84450288 -0.55668243  1.84450288 -0.55668243\n",
      "  1.84450288 -0.55668243  1.84450288 -0.55668243  1.84450288 -0.55668243\n",
      "  1.84450288 -0.55668243  1.84450288 -0.55668243  1.84450288 -0.55668243\n",
      "  1.84450288 -0.55668243  1.84450288 -0.55668243  1.84450288 -0.55668243\n",
      "  1.84450288 -0.55668243  1.84450288 -0.55668243  1.84450288 -0.55668243\n",
      "  1.84450288 -0.55668243  1.84450288 -0.55668243  1.84450288 -0.55668243\n",
      "  1.84450288 -0.55668243  1.84450288 -0.55668243  1.84450288 -0.55668243\n",
      "  1.84450288 -0.55668243  1.84450288 -0.55668243]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# トークン数\n",
    "N = 5\n",
    "# 埋め込みベクトルの次元\n",
    "D = 384\n",
    "\n",
    "\n",
    "input_data = [[i%2+1 for i in range(1,D+1)] for _ in range(N)]\n",
    "\n",
    "# 学習パラメータ\n",
    "gammas = [[np.random.normal() for _ in range(D)] for _ in range(N)]\n",
    "betas = [[np.random.normal() for _ in range(D)] for _ in range(N)]\n",
    "\n",
    "# 分母が0になるための無限小の数\n",
    "epsilon = 0.00000000001\n",
    "\n",
    "\n",
    "def ln(data, mu, sigma, i, j):\n",
    "    return gammas[i][j] * (data - mu)/np.sqrt(sigma**2 + epsilon) + betas[i][j]\n",
    "\n",
    "\n",
    "# 結果\n",
    "result = [[0 for _ in range(D)] for _ in range(N)]\n",
    "for i,x in enumerate(input_data):\n",
    "    # 平均\n",
    "    mu = np.mean(x)\n",
    "    # 標準偏差\n",
    "    sigma = np.std(x)\n",
    "    # Layer Normalization\n",
    "    for j,d in enumerate(x):\n",
    "        result[i][j] = ln(x, mu, sigma, i, j)\n",
    "\n",
    "# データの一部を表示\n",
    "print(result[0][0][0:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テンソルの形は$\\mathbb{R}^{N \\times D}$の中の数値を正規化しているだけのため、出力時も$\\mathbb{R}^{N \\times D}$である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MHSA(Multi-Head Self Attention)\n",
    "\n",
    "MHSAの全容は次の通り。\n",
    "\n",
    "![](images/summary/mhsa.png)\n",
    "\n",
    "Self Attentionの雰囲気をつかむコードを用意した。Self Attention自身を示すものではなく、あくまでも「自分自身を比較対象としてデータを生成する」というイメージをつかんでもらうものである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元のAのステータス18,a.sportiness=58,a.kindness=18\n",
      "新たなAのステータス45.449999999999996,new_a.sportiness=49.25,new_a.kindness=45.449999999999996\n"
     ]
    }
   ],
   "source": [
    "# self-attention の説明を読んで、雰囲気の例。Self-Attention自体を表すものではない。\n",
    "from random import seed, randint\n",
    "\n",
    "\n",
    "class Human:\n",
    "    def __init__(self, kindness, sportiness, smartness):\n",
    "        # 優しさ\n",
    "        self.kindness = kindness\n",
    "        # 格好良さ\n",
    "        self.sportiness = sportiness\n",
    "        # 頭の良さ\n",
    "        self.smartness = smartness\n",
    "\n",
    "seed(100)\n",
    "a = Human(randint(0, 100), randint(0, 100), randint(0, 100))\n",
    "b = Human(randint(0, 100), randint(0, 100), randint(0, 100))\n",
    "c = Human(randint(0, 100), randint(0, 100), randint(0, 100))\n",
    "d = Human(randint(0, 100), randint(0, 100), randint(0, 100))\n",
    "\n",
    "# a と a, b, c, dがどれくらい似ているかを数値化する\n",
    "# 特徴量 = 各Humanごとの慎重・体重・年齢を足した値\n",
    "# 類似度 = そのHumanの特徴量 / aの特徴量\n",
    "\n",
    "features = [ sum([h.__dict__[k] for k in h.__dict__]) for h in (a, b, c, d) ]\n",
    "\n",
    "# 乱数を固定しているので、feature = [279, 303, 304, 337]\n",
    "# aに一番近いのは a でその次に　aに近いのは b。\n",
    "\n",
    "# 一番近いaを60%、二番目近いbを30%、あとの二つを5%の重みづけでaを更新する\n",
    "\n",
    "new_a = Human(\n",
    "    sportiness=0.6*a.sportiness+0.3*b.sportiness+0.05*(c.sportiness + d.sportiness),\n",
    "    kindness=0.6*a.kindness+0.3*b.kindness+0.05*(c.kindness + d.kindness),\n",
    "    smartness=0.6*a.smartness+0.3*b.smartness+0.05*(c.smartness + d.smartness),\n",
    ")\n",
    "\n",
    "print(f\"元のAのステータス{a.kindness},{a.sportiness=},{a.kindness=}\")\n",
    "print(f\"新たなAのステータス{new_a.kindness},{new_a.sportiness=},{new_a.kindness=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query,Key,Valueを作成\n",
    "\n",
    "\n",
    "Layer Normalizationで正規化された$\\mathbb{R}^{N \\times D}$のデータから、**Query,Key,Value** という3つのデータを作成する。\n",
    "Query,Key,Valueを生成するには、入力されたデータにそれぞれの線形層$W_q, W_k, W_v$で埋め込む。\n",
    "QUery,Key,Valueはあとで合成するため、$W_q, W_k, W_v \\in \\mathbb{R}^{D \\times D_h} $というテンソル（行列）の形である。\n",
    "\n",
    "すなわち、入力データ$x \\in \\mathbb{R}^{N \\times D}$に対して、\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "q &= x W_q \\in \\mathbb{R}^{N \\times D_h}\\\\\n",
    "k &= x W_h \\in \\mathbb{R}^{N \\times D_h} \\\\\n",
    "v &= x W_k \\in \\mathbb{R}^{N \\times D_h} \n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "である。\n",
    "\n",
    "#### Query, Key, ValueをK個に分割\n",
    "\n",
    "ここでQuery,Key,ValueをそれぞれK個に分割する。分割しない場合は単純なSelf Attentionとなる。\n",
    "\n",
    "ここでデータの形に注意する。例としてQueryのデータをみる。\n",
    "$xW_q \\in \\mathbb{R}^{N \\in D}$は次のような形をしている。\n",
    "\n",
    "$xW_q =\n",
    "\\begin{pmatrix}\n",
    "q_1^{cls} & q_2^{cls} & \\dots & q_{D_h}^{cls}\\\\\n",
    "p_{1,1} & p_{1,2} & \\dots & p_{1,D_h}\\\\\n",
    "p_{2,1} & p_{2,2} & \\dots & p_{2,D_h}\\\\\n",
    "\\vdots & & \\ddots &\\\\\n",
    "p_{N_p,1} & p_{N_p,2} & \\dots & p_{N_p,D_h}\\\\\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "入力層では画像を$N_p$に分割し埋め込みなどの処理をして、クラストークンを追加した。\n",
    "そのため、上のようなデータの形となっている。つまり、クラストークンや各パッチのデータは行ごとに入っている。\n",
    "このデータを**列**でK個に分割する。つまり、分割した1つは次のような形になる。\n",
    "\n",
    "$\n",
    "(xW_q)_i\n",
    "\\begin{pmatrix}\n",
    "q_i^{cls} & q_{i+1}^{cls} & \\dots & q_{ i + \\frac{D_h}{K}-1}^{cls}\\\\\n",
    "p_{1,i} & p_{1,i+1} & \\dots & p_{1, i + \\frac{D_h}{K}-1}\\\\\n",
    "p_{2,i} & p_{2,i+1} & \\dots & p_{2,i + \\frac{D_h}{K}-1}\\\\\n",
    "\\vdots & & \\ddots &\\\\\n",
    "p_{N_p,1} & p_{N_p, i+1} & \\dots & p_{N_p, i + \\frac{D_h}{K}-1}\\\\\n",
    "\\end{pmatrix}\n",
    "\\quad i=1,2,\\dots,K\n",
    "$\n",
    "\n",
    "\n",
    "ややこしいので「形」に注目して説明をする。\n",
    "\n",
    "データは埋め込みで生成して $\\mathbb{R}^{N \\times D_h}$の状態である。\n",
    "これを列で$K$個に分割するとK個の行列ができ、各行列の列数は$\\frac{D_h}{K}$( $K \\times \\frac{D_h}{K} = D_h$ で元の列数)である。\n",
    "\n",
    "同様にKey,Valueでもそれぞれ同じ大きさの行列が$K$個生成される。\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "q_1, q_2, \\dots, q_K &\\in \\mathbb{R}^{N \\times \\frac{D_h}{K}}\\\\\n",
    "k_1, k_2, \\dots, k_K &\\in \\mathbb{R}^{N \\times \\frac{D_h}{K}}\\\\\n",
    "v_1, v_2, \\dots, v_K &\\in \\mathbb{R}^{N \\times \\frac{D_h}{K}}\\\\\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Query と Key の内積をとる\n",
    "\n",
    "QueryとKeyの内積をとる。計算としては行列の積である。\n",
    "\n",
    "ただし、「Keyの行列を転置して」かける。転置とは行列の行と列を置き換えることで、Keyの$i$番目の行列$k_i$の転置した行列を$k_i^T$と表現する。また、転置すると\n",
    "\n",
    "$\n",
    "k_1^T, k_2^T, \\dots, k_K^T \\in \\mathbb{R}^{\\frac{D_h}{K} \\times N}\n",
    "$\n",
    "\n",
    "の形である。\n",
    "\n",
    "するとQueryとKeyの$i$番目の$q_i$と$k_i^T$の積$q_i k_i^T$は$\\mathbb{R}^{N \\times \\frac{D_h}{K} }$ と $\\mathbb{R}^{\\frac{D_h}{K} \\times N}$とのかけ算であり、計算結果は $\\mathbb{R}^{N \\times N}$ の行列になる。\n",
    "これを$i=1, 2, \\dots,K$ごとに計算するため、$K$個できる。\n",
    "\n",
    "\n",
    "すなわち\n",
    "$q_1 k_1^T, q_1 k_1^T, \\dots, q_K k_K^T \\in \\mathbb{R}^{N \\times N}$\n",
    "である。\n",
    "\n",
    "\n",
    "#### SoftMax関数を適用する\n",
    "\n",
    "SoftMax関数は、Layer Normalizationのような正規化できる関数である。\n",
    "\n",
    "行列にSoftMax関数を適用することで、行列の各成分をすべて足し合わせると1になるように正規化する。\n",
    "計算の都合で、SoftMax関数を適用する前に$\\sqrt{D_h}$で割る。割る理由は、行列の値をある程度ならしてから\n",
    "適用するためである。\n",
    "\n",
    "行列の要素を正規化するだけのため、出力時の行列の大きさは$\\mathbb{R}^{n \\times n}$で変わらない。\n",
    "\n",
    "すなわち、次のようになる。\n",
    "\n",
    "\n",
    "$SoftMax( \\frac{q_1 k_1^T}{\\sqrt{D_h}} ),SoftMax( \\frac{q_2 k_2^T}{\\sqrt{D_h}} ),\\dots,SoftMax( \\frac{q_K k_K^T}{\\sqrt{D_h}} ) \\in \\mathbb{R}^{N \\times N}$\n",
    "\n",
    "\n",
    "#### DropOutする\n",
    "\n",
    "DropOutとは、一定の確率で使用しないようにすることである。\n",
    "\n",
    "今回の場合は、各$SoftMax(\\frac{q_i k_i^T}{\\sqrt{D_h} })$の中から、確率でいくつかの行列を無効（行列の要素をすべて0にするなど）にする。\n",
    "\n",
    "成分を操作するだけのため、行列の大きさや個数に変化はない。\n",
    "\n",
    "#### Valueと加重和する\n",
    "\n",
    "SoftMaxを適用した$q_i k_i^T$に、Valueの行列である$v_i$をかける。$\\mathbb{R}^{N \\times N}$と$\\mathbb{R}^{N \\times D_h}$のかけ算のため、\n",
    "出力される行列$SoftMax(\\frac{q_i k_i^T}{\\sqrt{D_h} }) v_i$ は$\\mathbb{R}^{N \\times D_h}$である。\n",
    "\n",
    "すなわち、\n",
    "\n",
    "$SoftMax( \\frac{q_1 k_1^T}{\\sqrt{D_h}} )v_1,SoftMax( \\frac{q_2 k_2^T}{\\sqrt{D_h}} )v_2,\\dots,SoftMax( \\frac{q_K k_K^T}{\\sqrt{D_h}} )v_K \\in \\mathbb{R}^{N \\times N}$\n",
    "\n",
    "である。\n",
    "\n",
    "\n",
    "#### 一つの行列にまとめる\n",
    "\n",
    "いままで生成した行列を横に並べて、一つの行列として考える。$K$個あるから、$\\mathbb{R}^{N \\times KN}$である。\n",
    "\n",
    "すなわち、\n",
    "\n",
    "$[SoftMax( \\frac{q_1 k_1^T}{\\sqrt{D_h}} )v_1,SoftMax( \\frac{q_2 k_2^T}{\\sqrt{D_h}} )v_2,\\dots,SoftMax( \\frac{q_K k_K^T}{\\sqrt{D_h}} )v_K] \n",
    "\\in \\mathbb{R}^{N \\times KN}$\n",
    "\n",
    "である。\n",
    "\n",
    "#### 行列の大きさを入力の大きさに揃える\n",
    "\n",
    "\n",
    "生成したデータは$\\mathbb{R}^{N \\times KN}$であるが、操作しやすい形にするために、線形層を適用して入力時の行列の大きさに整形する。\n",
    "\n",
    "線形層を$W^o \\in \\mathbb{R}^{KN \\times D}$とする。すると、$\\mathbb{R}^{N \\times KN}$ と $\\mathbb{R}^{KN \\times D}$ のかけ算なので、\n",
    "出力される行列は$\\mathbb{R}^{N \\times D}$である。\n",
    "\n",
    "すなわち、\n",
    "\n",
    "$[SoftMax( \\frac{q_1 k_1^T}{\\sqrt{D_h}} )v_1,SoftMax( \\frac{q_2 k_2^T}{\\sqrt{D_h}} )v_2,\\dots,SoftMax( \\frac{q_K k_K^T}{\\sqrt{D_h}} )v_K]W^o\n",
    "\\in \\mathbb{R}^{N \\times D}$\n",
    "\n",
    "である。\n",
    "\n",
    "#### まとめ\n",
    "\n",
    "今までに紹介した処理をまとめる。\n",
    "\n",
    "入力データ$z \\in \\mathbb{R}^{N \\times D}$からQuery,Key,Valueを線形層で適用して分割し、SoftMax関数を適用して$v_i$を加重和する関数を$SA_i(z)$とする(SAはSelf Attentionの意味である)。\n",
    "\n",
    "すなわち、\n",
    "\n",
    "$ SA_i(z) = SoftMax(\\frac{q_i k_i^T}{\\sqrt{D_h}})v_i \\quad (i=1, 2, \\dots, K)$\n",
    "\n",
    "である。\n",
    "\n",
    "更にそれらにValueを加重和して結合し、$W^o$で整形する関数を$MHSA(z)$とすると、次のように表現できる。\n",
    "\n",
    "$MHSA(z) = [SA_1(z); SA_2(z); \\dots; SA_K(z) ]W^o$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine1\n",
    "\n",
    "Input Layerのデータ$z$ と$MHSA(z)$を単純に足し合わせる。すなわち\n",
    "\n",
    "$z + MHSA(z) \\in \\mathbb{R}^{N \\times D}$\n",
    "\n",
    "である。\n",
    "\n",
    "### Layer Normalization2\n",
    "\n",
    "$z + MHSA(z)$ にLayer Normalizationを適用して、再度正規化する。\n",
    "\n",
    "$LN(z + MHSA(z)) \\in \\mathbb{R}^{N \\times D}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "MLPの全容は以下の通り。ただし、図は$LN(z + HHSA(z)) \\in \\mathbb{R}^{N \\times D}$ のうちの1列に対して適用した図である。\n",
    "\n",
    "![](images/summary/mlp.png)\n",
    "\n",
    "\n",
    "#### Linear1\n",
    "Linear1では、隠れ層を入力の行列の列数の4倍である$4D$で学習する。\n",
    "\n",
    "Linear1 の出力は $\\mathbb{R}^{N \\times 4D}$ である。\n",
    "\n",
    "\n",
    "#### GELU\n",
    "\n",
    "GELUは「Gaussian Error Linear Unit(ガウス誤差線形ユニット)」という活性化関数で、ReLUをふにゃっとした感じである。\n",
    "グラフは次のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/js/lwxty0zd1kjf9lrjh2v4rvx40000gn/T/ipykernel_22776/3583834801.py:14: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return x / 2 * (1 + np.tanh(np.sqrt( (2 / np.pi) * (x + 0.044715 * x ** 3) )))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def gelu(x):\n",
    "    return x * phi(x)\n",
    "\n",
    "def phi(x):\n",
    "    return x / 2 * (1 + np.tanh(np.sqrt( (2 / np.pi) * (x + 0.044715 * x ** 3) )))\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "relu_y = relu(x)\n",
    "gelu_y = gelu(x)\n",
    "\n",
    "re = sns.lineplot(relu_y, label=\"ReLU\")\n",
    "ge = sns.lineplot(gelu_y, label=\"GELU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GELU は単純な活性関数のため、出力は $\\mathbb{R}^{N \\times 4D}$ である。\n",
    "\n",
    "#### DropOut1\n",
    "\n",
    "DropOut1で、確率で無効にするパラメータを選択する。\n",
    "\n",
    "出力は $\\mathbb{R}^{N \\times 4D}$ である。\n",
    "\n",
    "\n",
    "#### Linear2\n",
    "\n",
    "2つめの線形層では、1つめの線形層で$4D$にしたのを$D$に戻す。\n",
    "\n",
    "出力は $\\mathbb{R}^{N \\times D}$ である。\n",
    "\n",
    "#### DropOut2\n",
    "\n",
    "DropOut2で更に、確率で無効にするパラメータを選択する。\n",
    "\n",
    "出力は $\\mathbb{R}^{N \\times D}$ である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最終処理\n",
    "\n",
    "最終処理の全容は次の通り。\n",
    "\n",
    "![](images/summary/output.png)\n",
    "\n",
    "入力の行列は$\\mathbb{R}^{N \\times D}$ のうち、最初の列のみ抽出する。\n",
    "\n",
    "そもそも、Input Layerでデータを作成したときを思い出すと、パッチを分割して埋め込んだベクトルの先頭に、埋め込みベクトルと同じ大きさのクラストークンを追加した。\n",
    "ViTではクラストークンのみを使用するため、最初の列だけを利用する。\n",
    "\n",
    "クラストークン$z_{CLS}$のベクトルは$\\mathbb{R}^{D}$である。\n",
    "\n",
    "#### MLP Head\n",
    "\n",
    "MLP Headの全容は次の通りである。\n",
    "\n",
    "![](images/summary/mlphead.png)\n",
    "\n",
    "MLP Head では、$x_{CLS}$ のベクトルをLayer Normalization で正規化する。\n",
    "Layer Normalizationはパラメータを正規化するだけのため、$x_{CLS}$の大きさは変わらず $\\mathbb{R}^{D}$である。\n",
    "\n",
    "その後は線形層を適用する。$L$を分類するクラス数とすると、 線形層の重みづけとして $W \\in \\mathbb{R}^{D \\times L}$の行列をかける。\n",
    "$x_{CLS}$は$\\mathbb{R}^{D}$のため、$W$をかけた $x_{CLS} W$ は$\\mathbb{R}^L$のベクトルである。\n",
    "\n",
    "この$x_{CLS}$によって、画像の分類をする。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21a447d43d212849e540c6c38cb6eb2460dc2380e24f4b0de591296981a71bc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
